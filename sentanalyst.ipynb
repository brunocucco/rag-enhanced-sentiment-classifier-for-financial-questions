{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2301fc8a-5fa7-4559-98be-126a1670d7d7",
   "metadata": {},
   "source": [
    "# GOAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b5793-0f59-4031-9cee-3e39237b31eb",
   "metadata": {},
   "source": [
    "Classify financial news snippets as positive, neutral, or negative. Model is trained on a huggingface dataset and a real-time analysis RAG pipeline is implemented for additional context.\n",
    "\n",
    "Dataset:\n",
    "financial_phrasebank, sentences_allagree. Several sentiment analysis context phrases.\n",
    "\n",
    "Model:\n",
    "distilbert-base-uncased. Effective and light-weight.\n",
    "\n",
    "Compute: \n",
    "Free-tier friendly (ml.m4.xlarge CPU or local).\n",
    "\n",
    "Framework: \n",
    "Hugging Face datasets + transformers (Trainer API)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3f50d2-634e-45d3-acd6-9f1be7ca3c4e",
   "metadata": {},
   "source": [
    "# User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "978b10eb-fd4d-404a-a3a0-863bf8b02717",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How is Nvidia is doing in 2025?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38e0fbf-aaf7-4f91-80c4-7b16018b295b",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97db657-c0f8-4afb-a745-ecca2cf23943",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"financial_phrasebank\", \"sentences_75agree\", trust_remote_code=True)\n",
    "\n",
    "# Shuffle dataset\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "# Train/Test/Split\n",
    "dataset = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# Check\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf5655-a0f9-4ab4-bc8a-0541cbc5d542",
   "metadata": {},
   "source": [
    "# Tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b3d22-fa20-4d0e-9d38-81392620fd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Apply tokenization\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "\n",
    "# Check first sample\n",
    "print(tokenized_dataset[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a37b7-ae00-4647-a919-9b627e108552",
   "metadata": {},
   "source": [
    "# Fine-tune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ddf3cc-45a3-4a93-8487-9b71da1143ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load model (3 labels: negative, neutral, positive)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",      \n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions, average=\"weighted\"),\n",
    "        \"precision\": precision_score(labels, predictions, average=\"weighted\"),\n",
    "        \"recall\": recall_score(labels, predictions, average=\"weighted\"),\n",
    "    }\n",
    "\n",
    "# Trainer object\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained(\"./finetuned-512-model\")\n",
    "tokenizer.save_pretrained(\"./finetuned-512-model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884acd81-af4d-47a3-b094-0c24346b6179",
   "metadata": {},
   "source": [
    "# Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e13d40-63c8-4767-a4e2-f9ac51baf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load model and tokenizer from local directory\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./finetuned-512-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./finetuned-512-model\")\n",
    "\n",
    "# Load the trained model and tokenizer into pipeline\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "examples = [\"The stock market is crashing.\"]\n",
    "\n",
    "for sentence in examples:\n",
    "    result = sentiment_pipeline(sentence)[0]\n",
    "    label_id = int(result['label'].split('_')[-1])\n",
    "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    print(f\"{sentence} â†’ {label_map[label_id]} (score: {result['score']:.2f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb2e59-732f-4f9b-a7c6-e7bd57a3b66d",
   "metadata": {},
   "source": [
    "# RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d0c18-f8be-4b50-b0f9-24eb8b7e1576",
   "metadata": {},
   "source": [
    "News Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532399a4-521c-42f9-983f-f219d002d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def clean_html(text):\n",
    "    text = re.sub(r'<a.*?>.*?</a>', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    return unescape(text.strip())\n",
    "\n",
    "def fetch_google_news_clean(query, max_results=200):\n",
    "    url = f\"https://news.google.com/rss/search?q={query.replace(' ', '+')}+when:7d&hl=en-US&gl=US&ceid=US:en\"\n",
    "    feed = feedparser.parse(url)\n",
    "\n",
    "    cleaned = []\n",
    "    for entry in feed.entries[:max_results]:\n",
    "        title = entry.title\n",
    "        summary = clean_html(entry.get(\"summary\", \"\"))\n",
    "        text = f\"{title}. {summary}\"\n",
    "        cleaned.append(text.strip())\n",
    "    \n",
    "    return cleaned\n",
    "\n",
    "# Test\n",
    "news = fetch_google_news_clean(query, max_results=200)\n",
    "for i, item in enumerate(news, 1):\n",
    "    print(f\"{i}. {item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c2e88f-ec55-402f-b4d7-0acd68a40c2b",
   "metadata": {},
   "source": [
    "Embed News with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419fc73-69f9-4fac-ac4a-e642b6a201e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Embedder (small, fast, free)\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Fetch news\n",
    "news = fetch_google_news_clean(query, max_results=200)\n",
    "\n",
    "# Step 2: Embed news headlines\n",
    "news_embeddings = embedder.encode(news, convert_to_numpy=True)\n",
    "\n",
    "# Step 3: Create FAISS structure\n",
    "dim = news_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(news_embeddings)\n",
    "\n",
    "# Step 4: Function to query similar headlines\n",
    "def retrieve_news(user_query, top_k=5):\n",
    "    query_vec = embedder.encode([user_query], convert_to_numpy=True)\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    return [(news[i], distances[0][j]) for j, i in enumerate(indices[0])]\n",
    "\n",
    "top_matches = retrieve_news(query)\n",
    "\n",
    "for i, (snippet, dist) in enumerate(top_matches, 1):\n",
    "    print(f\"{i}. [distance: {round(dist, 2)}] {snippet}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d6bab-74bf-4d83-91d4-84064549ebec",
   "metadata": {},
   "source": [
    "Acessing RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c9f30-efc8-43af-ba3e-3ee7d2563e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Build classifier\n",
    "sentiment_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "\n",
    "# Full RAG-enhanced classification\n",
    "def classify_with_rag(user_query, top_k=5):\n",
    "    # Retrieve relevant news\n",
    "    top_news = retrieve_news(user_query, top_k=top_k)\n",
    "    \n",
    "    # Format RAG context\n",
    "    context = \" \".join([snippet for snippet, _ in top_news])\n",
    "    \n",
    "    # Combine prompt + context as model input\n",
    "    full_input = f\"<NEWS>: {context} <QUERY>: {user_query}\"\n",
    "\n",
    "    # Run your classifier\n",
    "    pred = sentiment_pipeline(full_input)[0]\n",
    "    label_id = int(pred[\"label\"].split(\"_\")[-1])\n",
    "    label = label_map[label_id]\n",
    "    score = round(pred[\"score\"], 3)\n",
    "\n",
    "    return label, score, full_input\n",
    "\n",
    "label, score, full_input = classify_with_rag(query)\n",
    "\n",
    "print(f\"\\nðŸ§  Classified Query: '{query}'\")\n",
    "print(f\"ðŸ”Ž Result: {label} â€¢ {score}\")\n",
    "print(f\"\\nðŸ“Ž Model Input: {full_input[:200]}...\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
